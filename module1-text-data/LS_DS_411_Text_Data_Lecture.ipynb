{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LS_DS_411_Text_Data_Lecture.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "py37  (Python3)",
      "language": "python",
      "name": "py37"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "toc-autonumbering": false,
    "toc-showcode": false,
    "toc-showmarkdowntxt": false
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ryanleeallred/DS-Assignment-Submission-Practice-Repository/blob/master/module1-text-data/LS_DS_411_Text_Data_Lecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlwQRqffGEqb",
        "toc-hr-collapsed": false
      },
      "source": [
        "Lambda School Data Science\n",
        "\n",
        "*Unit 4, Sprint 1, Module 1*\n",
        "\n",
        "---\n",
        "<h1 id=\"moduleTitle\"> Natural Language Processing Introduction (Prepare)</h1>\n",
        "\n",
        "\"Natural\" meaning - not computer languages but spoken/written human languages. The hard thing about NLP is that human languages are far less structured or consistent than computer languages. This is perhaps the largest source of difficulty when getting computers to \"understand\" human languages. How do you get a machine to understand sarcasm, irony, synonyms, connotation, denotation, nuance, and tone of voice? All without it having lived a lifetime of experience for context? If you think about it, our human brains have been exposed to a lot of training data to help us interpret languages, and even then, we misunderstand each other pretty frequently. \n",
        "    \n",
        "\n",
        "<h2 id='moduleObjectives'>Learning Objectives</h2>\n",
        "\n",
        "By the end of this module, a student should be able to:\n",
        "* <a href=\"#p1\">Objective 1</a>: Tokenize text\n",
        "* <a href=\"#p1\">Objective 2</a>: Remove stop words from the text\n",
        "* <a href=\"#p3\">Objective 3</a>: Perform stemming and lemmatization on tokens\n",
        "\n",
        "## Conda Environments\n",
        "\n",
        "You will be completing each module in this sprint on your machine. We will be using conda environments to manage the packages and their dependencies for this sprint's content. In a classroom setting, instructors typically abstract away the environment for you. However, environment management is an important professional data science skill. We showed you how to manage environments using pipvirtual env during Unit 3, but in this sprint, we will introduce an environment management tool common in the data science community: \n",
        "\n",
        "> __conda__: Package, dependency, and environment management for any language—Python, R, Ruby, Lua, Scala, Java, JavaScript, C/ C++, FORTRAN, and more.\n",
        "\n",
        "The easiest way to install conda on your machine is via the [Anaconda Distribution](https://www.anaconda.com/distribution/) of Python & R. Once you have conda installed, read [\"A Guide to Conda Environments.\"](https://towardsdatascience.com/a-guide-to-conda-environments-bc6180fc533) This article will provide an introduction to some of the conda basics. If you need some additional help getting started, the official [\"Setting started with conda\"](https://conda.io/projects/conda/en/latest/user-guide/getting-started.html) guide will point you in the right direction. \n",
        "\n",
        ":snake: \n",
        "\n",
        "To get the sprint environment setup: \n",
        "\n",
        "1. Open your command-line tool (Terminal for MacOS, Anaconda Prompt for Windows)\n",
        "2. Navigate to the folder with this sprint's content. There should be a `requirements.txt`\n",
        "3. Run `conda create -n U4-S1-NLP python==3.7` => You can also rename the environment if you would like. Once the command completes, your conda environment should be ready.\n",
        "4. Now, we will add the required python packages for this sprint. You will need to 'activate' the conda environment: `source activate U4-S1-NLP` on Terminal or `conda activate U4-S1-NLP` on Anaconda Prompt. Once your environment is activated, run `pip install -r requirements.txt` which, will install the required packages into your environment.\n",
        "5. We are going to also add an Ipython Kernel reference to your conda environment so that we can use it from JupyterLab. \n",
        "6. Next, run `python -m ipykernel install --user --name U4-S1-NLP --display-name \"U4-S1-NLP (Python3)\"` => This will add a json object to an ipython file, so JupterLab will know that it can use this isolated instance of Python. :) \n",
        "7. In the last step, we need to install the models for Spacy. Run these commands `python -m spacy download en_core_web_md` and `python -m spacy download en_core_web_lg`\n",
        "8. Deactivate your conda environment and launch JupyterLab. You should now see \"U4-S1-NLP (Python3)\" in the list of available kernels on the launch screen. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Hty9Zb4kZ2c"
      },
      "source": [
        "# Dependencies for the week (instead of conda)\n",
        "# Run if you're using colab, otherwise you should have a local copy of the data\n",
        "#!wget https://raw.githubusercontent.com/LambdaSchool/DS-Unit-4-Sprint-1-NLP/main/requirements.txt\n",
        "# !pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "46khqvnMkZ2d"
      },
      "source": [
        "# You'll use en_core_web_sm for the sprint challenge due to memory constraints on Codegrader\n",
        "#!python -m spacy download en_core_web_sm \n",
        "\n",
        "# Locally (or on colab) let's use en_core_web_lg \n",
        "!python -m spacy download en_core_web_lg # Can do lg, takes awhile\n",
        "# Also on Colab, need to restart runtime after this step!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0ssyXeiGEqc",
        "toc-hr-collapsed": false
      },
      "source": [
        "# Tokenize Text (Learn)\n",
        "<a id=\"p1\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sd6cxaNTGEqc",
        "toc-hr-collapsed": true
      },
      "source": [
        "## Overview\n",
        "\n",
        "> **token**: an instance of a sequence of characters in some particular document that is grouped together as a useful semantic unit for processing\n",
        "\n",
        "> [_*Introduction to Information Retrieval*_](https://nlp.stanford.edu/IR-book/)\n",
        "\n",
        "\n",
        "### The Attributes of Good Tokens\n",
        "\n",
        "* Should be stored in an iterable data structure\n",
        "  - Allows analysis of the \"semantic unit\"\n",
        "* Should be all the same case\n",
        "  - Reduces the complexity of our data\n",
        "* Should be free of non-alphanumeric characters (i.e., punctuation, whitespace)\n",
        "  - Removes information that is probably not relevant to the analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvQPED-ys_mV"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXrgt7OJtAcK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZ7SSkwPs9zo"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDHLSlmUsesf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dK-EKGVNGEqd"
      },
      "source": [
        "Let's pretend we are trying to analyze the random sequence here. Question: what is the most common character in this sequence?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NODbGehhGEqe"
      },
      "source": [
        "random_seq = \"AABAAFBBBBCGCDDEEEFCFFDFFAFFZFGGGGHEAFJAAZBBFCZ\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uj0FHiJEGEqh"
      },
      "source": [
        "A useful unit of analysis for us is going to be a letter or character"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFWePC6XGEqh"
      },
      "source": [
        "tokens = list(random_seq)\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tbp-hyDGEql"
      },
      "source": [
        "Our tokens are already \"good\": in an iterable data structure, all the same case, and free of noise characters (punctuation, whitespace), so we can jump straight into the analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFQcACruGEql"
      },
      "source": [
        "import seaborn as sns\n",
        "\n",
        "sns.countplot(tokens);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3TbbxfHGEqo"
      },
      "source": [
        "The most common character in our sequence is  \"F.\" We can't just glance at the sequence to know which character is the most common. We (humans) struggle to subitize complex data (like random text sequences).\n",
        "\n",
        "> __Subitize__ is the ability to tell the number of objects in a set quickly, without counting.  \n",
        "\n",
        "We need to chunk the data into countable pieces \"tokens\" for us to analyze them. This inability to subitize text data is the motivation for our discussion today."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMa8NJjlGEqo",
        "toc-hr-collapsed": true
      },
      "source": [
        "### Tokenizing with Pure Python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "im96HX4XGEqp"
      },
      "source": [
        "sample = \"Friends, Romans, countrymen, lend me your ears;\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8ACUekrGEqr"
      },
      "source": [
        "##### Iterable Tokens\n",
        "\n",
        "A string object in Python is already iterable. However, the item you iterate over is a character not a token:\n",
        "\n",
        "```\n",
        "from time import sleep\n",
        "for num, character in enumerate(sample):\n",
        "    sleep(.5)\n",
        "    print(f\"Char {num} - {character}\", end=\"\\r\")\n",
        "```\n",
        "\n",
        "Instead of caring about the words in our sample (our semantic unit), we can use the string method `.split()` to separate the whitespace and create iterable units. :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5Vh69V5GEqr"
      },
      "source": [
        "sample.split(\" \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3h3fMFY0GEqu"
      },
      "source": [
        "##### Case Normalization\n",
        "A common data cleaning task with token is to standardize or normalize the case. Normalizing case reduces the chance of duplicate records for things that have practically the same semantic meaning. You can use either the `.lower()` or `.upper()` string methods to normalize case.\n",
        "\n",
        "Consider the following example: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2K43cyJGEqu"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('./data/Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products_May19.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFE7eMaskZ2k"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "id": "vbsvd0VvGEqx",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "e986ee4afc96df48ac674f9b99732272",
          "grade": false,
          "grade_id": "cell-a170e7dda094d54e",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "source": [
        "# Get the count of how many times each unique brand occurs \n",
        "\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkhFYsNXGEq1"
      },
      "source": [
        "##### Keep Only Alphanumeric Characters\n",
        "Yes, we only want letters and numbers. Everything else is probably noise: punctuation, whitespace, and other notation. This one is a little bit more complicated than our previous example. Here we will have to import the base package `re` (regular expressions). \n",
        "\n",
        "The only regex expression pattern you need for this is `'[^a-zA-Z 0-9],'` which keeps lower case letters, upper case letters, spaces, and numbers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRY7kKSAGEq4"
      },
      "source": [
        "sample = sample +\" 911\"\n",
        "print(sample)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d001HholkZ2l"
      },
      "source": [
        "import re\n",
        "\n",
        "re.sub('[^a-zA-Z 0-9]', '', sample)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obloLh7rGEq7"
      },
      "source": [
        "#### Two Minute Challenge \n",
        "- Complete the function `tokenize` below\n",
        "- Combine the methods which we discussed above to clean text before we analyze it\n",
        "- You can put the methods in any order you want"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "id": "0zgbOnoIGEq7",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "0f09207520f5a3a425756889e9cf78aa",
          "grade": false,
          "grade_id": "cell-42630c1891924a1a",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "source": [
        "def tokenize(text):\n",
        "    \"\"\"Parses a string into a list of semantic units (words)\n",
        "\n",
        "    Args:\n",
        "        text (str): The string that the function will tokenize.\n",
        "\n",
        "    Returns:\n",
        "        list: tokens parsed out by the mechanics of your choice\n",
        "    \"\"\"\n",
        "    \n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()\n",
        "    return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWsYy-LqGEq9"
      },
      "source": [
        "# this should be your output\n",
        "tokenize(sample)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erSMd4diGEq_",
        "toc-hr-collapsed": true
      },
      "source": [
        "## Follow Along\n",
        "\n",
        "Our inability to analyze text data becomes quickly amplified in a business context. Consider the following: \n",
        "\n",
        "A business that sells widgets also collects customer reviews of those widgets. When the business first started, they had a human read the reviews to look for patterns. Now, the business sells thousands of widgets a month. The human readers can't keep up with the pace of reviews to synthesize an accurate analysis. They need some science to help them analyze their data.\n",
        "\n",
        "Now, let's pretend that business is Amazon, and the widgets are Amazon products such as the Alexa, Echo, or other AmazonBasics products. Let's analyze their reviews with some counts. This dataset is available on [Kaggle](https://www.kaggle.com/datafiniti/consumer-reviews-of-amazon-products/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ap1zL81GErA"
      },
      "source": [
        "\"\"\"\n",
        "Import Statements\n",
        "\"\"\"\n",
        "\n",
        "# Base\n",
        "from collections import Counter\n",
        "import re\n",
        " \n",
        "import pandas as pd\n",
        "\n",
        "# Plotting\n",
        "import squarify\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# NLP Libraries\n",
        "import spacy\n",
        "from spacy.tokenizer import Tokenizer\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "nlp = spacy.load('en_core_web_lg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydhFysF-GErC"
      },
      "source": [
        "df.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "id": "Lkl_l_3KGErH",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "c9af223b0b92d4f38bf65a8a59b77553",
          "grade": false,
          "grade_id": "cell-afe39f461a4852ac",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "source": [
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hOBAw2yGErU"
      },
      "source": [
        "#### Analyzing Tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "id": "6jVvZAvJGErU",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "4fcf360b68c204e6742580d513c4239e",
          "grade": false,
          "grade_id": "cell-1df54ac52c426166",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "source": [
        "# Object from Base Python\n",
        "from collections import Counter\n",
        "\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiVHbw6xGErW"
      },
      "source": [
        "Let’s create a function that takes a corpus of documents and returns a data frame of word counts to analyze."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypyH-_x1GErX"
      },
      "source": [
        "def count(tokens):\n",
        "    \"\"\"\n",
        "    Calculates some basic statistics about tokens in our corpus (i.e., corpus means collections text data)\n",
        "    \"\"\"\n",
        "    # stores the count of each token\n",
        "    word_counts = Counter()\n",
        "    \n",
        "    # stores the number of docs that each token appears in \n",
        "    appears_in = Counter()\n",
        "\n",
        "    total_docs = len(tokens)\n",
        "\n",
        "    for token in tokens:\n",
        "        # stores count of every appearance of a token \n",
        "        word_counts.update(token)\n",
        "        # use set() in order to not count duplicates, thereby count the num of docs that each token appears in\n",
        "        appears_in.update(set(token))\n",
        "\n",
        "    # build word count dataframe\n",
        "    temp = zip(word_counts.keys(), word_counts.values())\n",
        "    wc = pd.DataFrame(temp, columns = ['word', 'count'])\n",
        "\n",
        "    # rank the the word counts\n",
        "    wc['rank'] = wc['count'].rank(method='first', ascending=False)\n",
        "    total = wc['count'].sum()\n",
        "\n",
        "    # calculate the percent total of each token\n",
        "    wc['pct_total'] = wc['count'].apply(lambda token_count: token_count / total)\n",
        "\n",
        "    # calculate the cumulative percent total of word counts \n",
        "    wc = wc.sort_values(by='rank')\n",
        "    wc['cul_pct_total'] = wc['pct_total'].cumsum()\n",
        "\n",
        "    # create dataframe for document stats\n",
        "    t2 = zip(appears_in.keys(), appears_in.values())\n",
        "    ac = pd.DataFrame(t2, columns=['word', 'appears_in'])\n",
        "    \n",
        "    # merge word count stats with doc stats\n",
        "    wc = ac.merge(wc, on='word')\n",
        "\n",
        "    wc['appears_in_pct'] = wc['appears_in'].apply(lambda x: x / total_docs)\n",
        "\n",
        "    return wc.sort_values(by='rank')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqqwygrUGErZ"
      },
      "source": [
        "# Use the Function\n",
        "wc  = count(df['tokens'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0EkCpReGEra"
      },
      "source": [
        "wc.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9kI5BjnGErc"
      },
      "source": [
        "import seaborn as sns\n",
        "\n",
        "# Cumulative Distribution Plot\n",
        "sns.lineplot(x='rank', y='cul_pct_total', data=wc);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKpixh5DGEre"
      },
      "source": [
        "wc[wc['rank'] <= 100]['cul_pct_total'].max()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yuCq8nuGErg"
      },
      "source": [
        "import squarify\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "wc_top20 = wc[wc['rank'] <= 20]\n",
        "\n",
        "squarify.plot(sizes=wc_top20['pct_total'], label=wc_top20['word'], alpha=.8 )\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIXSsYI_GEri"
      },
      "source": [
        "### Processing Raw Text with Spacy\n",
        "\n",
        "Spacy's data model for documents is unique among NLP libraries. Instead of storing the document's components in various data structures, Spacy indexes components and stores the lookup information. \n",
        "\n",
        "This is why Spacy is considered to be more-production grade than a library like NLTK."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0l_8Q_-GEri"
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_lg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qlRt5SIGErk"
      },
      "source": [
        "sample = \"\"\"\n",
        "Natural Language Processing Summary\n",
        "The field of study that focuses on the interactions between human language and computers is called Natural Language Processing, or NLP for short. It sits at the intersection of computer science, artificial intelligence, and computational linguistics (Wikipedia).\n",
        "\n",
        "“Nat­ur­al Lan­guage Pro­cessing is a field that cov­ers com­puter un­der­stand­ing and ma­nip­u­la­tion of hu­man lan­guage, and it’s ripe with pos­sib­il­it­ies for news­gath­er­ing,” Anthony Pesce said in Natural Language Processing in the kitchen. “You usu­ally hear about it in the con­text of ana­lyz­ing large pools of legis­la­tion or other doc­u­ment sets, at­tempt­ing to dis­cov­er pat­terns or root out cor­rup­tion.”\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjA1TF9vGErl"
      },
      "source": [
        "doc = nlp(sample)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "id": "ZBa99HsYGErn",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "81b7eb7803a81637e2acde6779d1452a",
          "grade": false,
          "grade_id": "cell-fc63dad59954bbff",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "source": [
        "# create a tokenizer using spacy\n",
        "\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FS-IuYRdGEr3",
        "toc-hr-collapsed": true
      },
      "source": [
        "## Challenge\n",
        "\n",
        "In the module project, you will apply tokenization to another set of review data and visualize those tokens. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qy-JQ3VUGEr6",
        "toc-hr-collapsed": true
      },
      "source": [
        "# Stop Words (Learn)\n",
        "<a id=\"p2\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APfLF769GEr6"
      },
      "source": [
        "## Overview\n",
        "Section Agenda\n",
        "- What are they?\n",
        "- How do we get rid of them using Spacy?\n",
        "- Visualization\n",
        "- Libraries of Stop Words\n",
        "- Extending Stop Words\n",
        "- Statistical trimming\n",
        "\n",
        "If the visualizations above, you began to notice a pattern. Most of the words don't add much to our understanding of product reviews. Words such as \"I,\" \"and,\" \"of,\" etc., have almost no semantic meaning to us. We call these useless words \"stop words\" because we should 'stop' ourselves from including them in the analysis. \n",
        "\n",
        "Most NLP libraries have built-in lists of stop words that are common English: conjunctions, articles, adverbs, pronouns, and common verbs. However, the best practice is to extend/customize these standard English stopwords for your problem’s domain. For example, if I am studying political science, I may want to exclude the word “politics” from my analysis; it’s so common it does not add to my understanding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b5yWU9hGEr7",
        "toc-hr-collapsed": true
      },
      "source": [
        "## Follow Along \n",
        "\n",
        "### Default Stop Words\n",
        "Let's take a look at the standard stop words that came with our Spacy model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "zeOqLNAhGEr7",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "source": [
        "# Spacy's Default Stop Words\n",
        "nlp.Defaults.stop_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "id": "gYnQfN9TGEr9",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "659d58e18d2e1352236109ea3597cd08",
          "grade": false,
          "grade_id": "cell-6786f165c68a0aa6",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "source": [
        "# Use spacy to create a tokenizer that removes stop words\n",
        "\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wW7qoSu_GEsD"
      },
      "source": [
        "### Extending Stop Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxLUxpRvGEsD"
      },
      "source": [
        "print(type(nlp.Defaults.stop_words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7D6R_35SGEsF"
      },
      "source": [
        "STOP_WORDS = nlp.Defaults.stop_words.union(['batteries','I', 'amazon', 'i', 'Amazon', 'it', \"it's\", 'it.', 'the', 'this',])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "id": "0JOoLRyUGEsJ",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "068a4f6cd4bebc5c8cbc74372c41a905",
          "grade": false,
          "grade_id": "cell-37cd0e460b63ff43",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "source": [
        "# use spacy to create a tokenizer that removes stopwords using STOP_WORDS\n",
        "\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55BKEXW6GEsQ"
      },
      "source": [
        "### Statistical Trimming\n",
        "\n",
        "So far, we have talked about stop word in relation to either broad English words or domain-specific stop words. Another common approach to stop word removal is via statistical trimming. The basic idea: preserve the words that give the most about of variation in your data. \n",
        "\n",
        "Do you remember this graph?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7M5bXwUGEsQ"
      },
      "source": [
        "sns.lineplot(x='rank', y='cul_pct_total', data=wc);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSY5DHLKGEsS"
      },
      "source": [
        "This graph tells us that only a *handful* of words represented 80% of words in the overall corpus. We can interpret this in two ways: \n",
        "1. The most frequent words may not provide any insight into the mean on the documents since they are so prevalent. \n",
        "2. Words that appear in frequency (at the end of the graph) also probably do not add much value because they are mentioned so rarely. \n",
        "\n",
        "Let's take a look at the words at the bottom and the top and make a decision for ourselves:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evsmVP2VGEsV"
      },
      "source": [
        "# Frequency of appears in documents\n",
        "sns.distplot(wc['appears_in_pct']);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJkyvXn0GEsX"
      },
      "source": [
        "# Tree-Map w/ Words that appear in at least 2.5% of documents. \n",
        "\n",
        "wc = wc[wc['appears_in_pct'] >= 0.025]\n",
        "\n",
        "sns.distplot(wc['appears_in_pct']);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riFOSWTuGEsa"
      },
      "source": [
        "## Challenge\n",
        "\n",
        "In the module project, you will apply to stop word removal to a new corpus. You will focus on using dictionary-based stop word removal, but you should consider applying statistical stopword trimming as a stretch goal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xh6NpgGhGEsa",
        "toc-hr-collapsed": true
      },
      "source": [
        "# Stemming & Lemmatization (Learn)\n",
        "<a id=\"p3\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82BICah4GEsa",
        "toc-hr-collapsed": false
      },
      "source": [
        "## Overview\n",
        "\n",
        "You can see from our example above there is still some normalization to do to get a clean analysis. You notice that there are many words (*i.e.,* 'batteries,' 'battery') that share the same root word. We can use either the process of stemming or lemmatization to trim our words down to the 'root' word. \n",
        "\n",
        "__Section Agenda__:\n",
        "\n",
        "- Which is which\n",
        "- why use one v. other\n",
        "- show side by side visualizations \n",
        "- how to do it in spacy & nltk\n",
        "- introduce PoS in here as well"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Unc5QrF4GEsb",
        "toc-hr-collapsed": true
      },
      "source": [
        "## Follow Along"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2qObh4HGEsb",
        "toc-hr-collapsed": true
      },
      "source": [
        "### Stemming\n",
        "\n",
        "> *a process for removing the commoner morphological and inflexional endings from words in English. Its main use is as part of a term normalisation process that is usually done when setting up Information Retrieval systems.* - [Martin Porter](https://tartarus.org/martin/PorterStemmer/)\n",
        "\n",
        "Some examples include:\n",
        "- 'ing'\n",
        "- 'ed'\n",
        "- 's'\n",
        "\n",
        "These rules are by no means comprehensive, but they are somewhere to start. Most stemming is done by well-documented algorithms such as Porter, Snowball, and Dawson. Porter and its newer version Snowball are the most popular stemming algorithms today. For more information on various stemming algorithms, check out [*\"A Comparative Study of Stemming Algorithms.\"*](https://pdfs.semanticscholar.org/1c0c/0fa35d4ff8a2f925eb955e48d655494bd167.pdf) \n",
        "\n",
        "\n",
        "Spacy does not do stemming out of the box but instead uses a different technique called *lemmatization* which we will discuss in the next section. Let's turn to an antique python package `nltk` for stemming. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7z49DWNGEsb"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "ps = PorterStemmer()\n",
        "\n",
        "words = [\"is\", \"was\", \"be\", \"are\"]\n",
        "\n",
        "for word in words:\n",
        "    print(ps.stem(word))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkPTDh-dGEsd"
      },
      "source": [
        "### Two Minute Challenge\n",
        "\n",
        "Apply the Porter stemming algorithm to the tokens in the `df` data frame. Visualize the results in the tree graph we have been using for this session."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "id": "sCrtEa_uGEsd",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "492b5c13a834b2ce914c350efb369854",
          "grade": false,
          "grade_id": "cell-5944a38ffd47803f",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "source": [
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HU96W_PnGEsg",
        "toc-hr-collapsed": false
      },
      "source": [
        "### Lemmatization\n",
        "\n",
        "You notice immediately that results are kinda funky - words just oddly chopped off. The Porter algorithm did exactly what it knows to do: chop off endings. Stemming works well in applications where humans don't have to worry about reading the results. Search engines and, more broadly, information retrieval algorithms use stemming. Why? Because it's fast. \n",
        "\n",
        "Lemmatization, on the other hand, is more methodical. The goal is to transform a word into its base form called a lemma. Plural nouns with funky spellings get transformed to singular tense. Verbs are all transformed to the transitive-nice tidy data for visualization. :) However, this tidy data can come at a computational cost. Spacy does a pretty freaking good job of it, though. Let's take a look:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1uvFqG9GEsg"
      },
      "source": [
        "sent = \"men man women woman wolf wolves run runs running\"\n",
        "doc = nlp(sent)\n",
        "\n",
        "# Lemma Attributes\n",
        "for token in doc:\n",
        "    print(token.text, \"  \", token.lemma_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "id": "sjt7J4RsGEsh",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "ca7ddfb773402c65d8f76dd921bd3389",
          "grade": false,
          "grade_id": "cell-792ac177c78518bc",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "source": [
        "# Wrap it all in a function\n",
        "def get_lemmas(text):\n",
        "\n",
        "    lemmas = []\n",
        "    \n",
        "    doc = nlp(text)\n",
        "    \n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()\n",
        "    \n",
        "    return lemmas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0RL6T5OGEsi"
      },
      "source": [
        "df['lemmas'] = df['reviews.text'].apply(get_lemmas)  # Might take a few minutes!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V50IDI9_GEsj"
      },
      "source": [
        "cols = ['lemmas', 'reviews.text']\n",
        "df[cols].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4FNmpcNGEsl"
      },
      "source": [
        "wc = count(df['lemmas'])\n",
        "wc_top20 = wc[wc['rank'] <= 20]\n",
        "\n",
        "squarify.plot(sizes=wc_top20['pct_total'], label=wc_top20['word'], alpha=.8 )\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_0hfUbGGEsn"
      },
      "source": [
        "## Challenge\n",
        "\n",
        "You should know how to apply lemmatization with Spacy to a corpus of text. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TV0MV3qqGEso"
      },
      "source": [
        "# Review\n",
        "\n",
        "In this module project, you've seen us apply Natural Language Processing techniques (tokenization, stopword removal, and lemmatization) to a corpus of Amazon text reviews. We analyzed those reviews using these techniques and discovered that Amazon customers are generally satisfied with the battery life of Amazon products and appear typically satisfied. \n",
        "\n",
        "You will apply similar techniques to today's module project assignmentto analyze coffee shop reviews from yelp. Remember that the techniques of processing the text are just the beginning. There are many ways to slice and dice the data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sf2wyGutGEso"
      },
      "source": [
        "# Sources\n",
        "\n",
        "* Spacy 101 - https://course.spacy.io\n",
        "* NLTK Book - https://www.nltk.org/book/\n",
        "* An Introduction to Information Retrieval - https://nlp.stanford.edu/IR-book/pdf/irbookonlinereading.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIXFuZCEGEso",
        "toc-hr-collapsed": true
      },
      "source": [
        "## Advanced Resources & Techniques\n",
        "- Named Entity Recognition (NER)\n",
        "- Dependency Trees \n",
        "- Generators\n",
        "- the major libraries (NLTK, Spacy, Gensim)"
      ]
    }
  ]
}